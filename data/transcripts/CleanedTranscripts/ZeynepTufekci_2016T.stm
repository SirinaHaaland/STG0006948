  i started my first job as a computer programmer in my very first year of college basically as a teenager  
 soon after i started working writing software in a company 
 a manager who worked at the company came down to where i was and he whispered to me
  can he tell if i 'm lying there was nobody else in the room 
 can who tell if you 're lying and why are we whispering 
 the manager pointed at the computer in the room can he
 tell if i 'm lying well  that manager was having an affair with the receptionist
  and i was still a teenager  so i whisper shouted back to him yes the computer can tell if you 're lying well
 i laughed but actually the laugh 's on me  nowadays  there are computational systems that can suss out emotional states and even lying from
 from processing human faces  advertisers and even governments are very interested 
 i had become a computer programmer because i was one of those kids crazy about math and science  
 but somewhere along the line i 'd learned about nuclear weapons and i 'd gotten really concerned with the ethics of science i was troubled  
 however because of family circumstances  i also needed to start working as soon as possible  so i thought to myself hey let me pick a technical field where i can
  a job easily and where i don 't have to deal with any troublesome questions of ethics  so i picked computers 
 laughter well ha ha ha all the laughs are on me  nowadays computer scientists are building platforms that control what
 a billion people see every day  they 're developing cars that could decide who to run over
  even building machines weapons that might kill human beings in war it 's ethics all the way down 
 machine intelligence is here we
 're now using computation to make all sort of decisions  but also new kinds of decisions  we 're asking questions to computation that have no
  we 're asking questions like who should the company hire 
 which update from which friend should you be shown which convict is more likely to reoffend which news item or movie should be recommended to people 
 look  yes we 've been using computers for a while but this is different this is a historical twist
  because we cannot anchor computation for such subjective decisions the way we can anchor computation for
 flying airplanes building bridges going to the moon 
 are airplanes safer did the bridge sway and fall 
 there we have agreed upon fairly clear benchmarks and we have laws of nature to guide us we have no such anchors and benchmarks for
 decisions in messy
  human affairs to make things more complicated our software is getting more powerful  but it 's also getting less transparent and more complex 
 recently in the past decade complex algorithms have made great strides  they can recognize human faces  
 they can decipher handwriting  they can detect credit card fraud and block spam and they can translate between languages they can
  detect tumors in medical imaging  they can beat humans in chess and go 
 much of this progress comes from a method called machine learning 
 machine learning is different than traditional programming where you give the computer detailed exact painstaking instructions  
 it 's more like you take the system and you feed it lots of data including unstructured data like the kind we generate in our digital lives
 and the system learns by churning through this data 
 and also crucially  
 these systems don 't operate under a single answer logic  they don 't produce a simple answer it 's more probabilistic this one is probably more like what you 're looking for 
 now the upside is this method is really powerful the head of google 's ai systems called it the unreasonable effectiveness of data
  downside is we don 't really understand what the system learned in fact that 's its power this is less like giving instructions to a
 computer  it 's more like training
  we don 't really understand or control  so this is our problem  it 's a problem when this artificial intelligence system gets things wrong
  also a problem when it gets things right because we don 't even know which is which when it 's a subjective problem  we don 't know what this thing is thinking 
 so  consider a hiring algorithm
 a system used to hire people  using machine learning systems such a system would have been trained on previous employees ' data
 and instructed to
  find and hire people like the existing high performers in the company 
 sounds good 
 i once attended a conference that brought together human resources managers and executives high level people using such systems in hiring they were super excited  they thought that this would make hiring more objective 
  give women and minorities a better shot against biased human managers and look
 human hiring is biased i know i
 mean in one of my early jobs as a programmer my immediate manager would sometimes come down to where i was really early in the morning or really late in the afternoon
  and she 'd say 
 zeynep let 's go to lunch
 so free lunch i always went  
 i later realized what was happening my immediate managers had not confessed to their higher ups that the programmer they hired for a serious job was a teen girl
 who wore jeans and sneakers to work  i was doing a good job i just looked wrong and was the wrong age
  computational systems can infer all sorts of things about you from your digital crumbs even if you have not disclosed those things  
 they can infer
 your sexual orientation your personality traits your political leanings  they have predictive power
  power with high levels of accuracy  
 remember for things you haven 't even disclosed this is inference  
 i have a friend who developed such computational systems to predict the likelihood of clinical or postpartum depression from social media data 
 the results are impressive her system can predict the likelihood of depression months before the onset of any
  symptoms months before no symptoms there 's prediction she hopes it will be used for
 early intervention great  but now put this in the context of hiring  
 so at this human resources managers conference i approached a high level manager in a very large
  what if unbeknownst to you your system
 is weeding out people with high future likelihood of depression  
 they 're not depressed now just maybe in the future more likely 
 what if it 's weeding out women
 more likely to be pregnant in the next year or two but aren 't pregnant now 
 what if it 's hiring aggressive people because that 's your workplace culture
  you can 't tell this by looking at gender breakdowns those may be balanced  and since this is machine learning 
 not traditional coding there is no variable there labeled higher risk of depression higher risk of pregnancy aggressive guy scale 
 not only do you not know what your system is selecting
  you don 't even know where to begin to look  it 's a black box it has predictive power  but you don 't understand it 
 what safeguards i asked do you have to make sure that your black box isn 't doing something shady 
 she looked at me as if i had just stepped on ten puppy tails 
 laughter she stared at me and she said i don 't
 want to hear another word about this
  away mind you she wasn 't rude  it was clearly what i don 't know isn 't my problem go away  death stare 
 look such a system may even be less biased than human managers in some ways and it could make monetary sense  
 but it could also lead to a steady but stealthy shutting out
  of the job market of people with higher risk of depression 
 is this the kind of society we want to build without even knowing we 've done this because we turned  decision making to machines we don 't totally understand 
 another problem is this these systems are often trained on data generated by our actions 
  could just be reflecting our biases and these systems could be picking up on our biases and amplifying them and showing them back to us while we 're telling ourselves we
 're just doing objective neutral computation 
 researchers found that on google 
 women are less likely than men to be shown job ads for high paying jobs and searching for
 african american names is more likely to bring up ads suggesting criminal history even when there is none 
 such hidden biases and black box algorithms
 that researchers uncover sometimes but sometimes we don 't know can have life altering consequences  
  a defendant was sentenced to six years in prison for evading the police 
 you may not know this but algorithms are increasingly used in parole and sentencing decisions he wanted to know how is this score calculated  
 it 's a commercial black box the company refused to have its algorithm be challenged in open court 
 but propublica an investigative nonprofit audited that very algorithm with what public data they could find
  and found that its outcomes were biased and its predictive power was dismal barely better than chance  and it was wrongly labeling black defendants as future criminals 
 at twice the rate of white defendants 
  down the street with a friend of hers they spotted an unlocked kid 's bike and a scooter on a porch and foolishly jumped on it as they were speeding off a woman came out and said hey  that 's my kid 's bike 
 they dropped it they walked away  but they were arrested  she was wrong she was foolish but she was also just eighteen she
  that man had been arrested for shoplifting in home depot  eighty five dollars ' worth of stuff a similar petty crime
  but he had two prior armed robbery convictions  
 but the algorithm scored her as high risk and not him 
 two years later propublica found that she had not reoffended
  was just hard to get a job for her with her record he on the other hand did reoffend and is now serving an eight year prison term for a later crime 
 clearly  we need to audit our black boxes and not have them have this kind of unchecked power 
 audits are great and important but they don 't solve all our problems take facebook 's powerful news feed algorithm you know the one that ranks everything and decides
  show you from all the friends and pages you follow should you be shown another baby picture 
 a sullen note from an acquaintance an important but difficult news item  there 's no right answer 
 facebook optimizes for engagement on the site likes shares comments
 in august of two thousand and fourteen protests broke out in ferguson missouri
 after the killing
  teenager by a white police officer under murky circumstances  
 the news of the protests was all over my algorithmically unfiltered twitter feed but nowhere on my facebook 
 was it my facebook friends i disabled facebook 's algorithm 
 which is hard because facebook keeps wanting to make you
 come under the algorithm 's control and saw that my friends were talking about
 the algorithm wasn 't showing it to me  i researched this and found this was a widespread problem  the story of ferguson wasn 't
 it 's not likable who 's going to click on like 
 it 's not even easy to comment on without likes and comments the algorithm was likely showing it to even fewer people
  get to see this 
 instead that week facebook 's algorithm highlighted this which is the als ice bucket challenge  worthy cause dump ice water donate to charity fine  but it was super algorithm friendly 
 the machine made this decision for us 
 a very important but difficult conversation might have been smothered had facebook been the only
  these systems can also be wrong in ways that don 't resemble human systems do you guys remember watson
  system that wiped the floor with human contestants on jeopardy it was a great player  but then for final jeopardy  watson was asked this question 
 its largest airport is named for a world war ii hero its second largest for a world war ii battle
  watson on the other hand answered toronto for a us city category  
 the impressive system also made an error that a human
 would never make a second grader wouldn 't make our machine intelligence can fail in ways that don 't fit error patterns of humans 
  in ways we won 't expect and be prepared for 
 it 'd be lousy not to get a job one is qualified for  
 but it would triple suck if it was because of stack overflow in some subroutine laughter
 in may of two thousand and ten 
 a flash crash on wall street fueled by a feedback loop in wall street 's 
 sell algorithm wiped a trillion dollars of
  i don 't even want to think what error means in the context of lethal autonomous weapons  
 so yes humans have always
 made biases decision makers and gatekeepers in courts in news 
 in war  they make mistakes but that 's exactly my point we cannot escape these difficult questions 
 we cannot outsource
  our responsibilities to machines 
 artificial intelligence does not give us a get out of ethics free card 
 data scientist fred benenson calls this math washing we need the opposite  we need
  we need to make sure we have algorithmic accountability  auditing and meaningful transparency  we need to accept that bringing math and computation to messy 
 human affairs does not bring objectivity rather the complexity of human affairs invades the algorithms 
 yes  we can and we should use computation to help us make better decisions but
  have to own up to our moral responsibility to judgment and use algorithms within that framework not as a means to abdicate 
 and outsource our responsibilities to one another as human to human 
