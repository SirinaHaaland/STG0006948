  our communication with machines has always been limited to conscious and direct forms 
 whether it 's something simple like turning on the lights with a switch or even as complex as programming robotics we
  have always had to give a command to a machine or even a series of commands in order for it to do something for us  
 communication between people on the other hand 
 is far more complex and a lot more interesting because we take into account so much more than what is explicitly expressed 
 we observe facial expressions body
  and we can intuit feelings and emotions from our dialogue with one another this actually forms a large part of our decision making process 
 our vision is to introduce this whole new realm of human interaction
 into human computer interaction
 so that computers can understand not only what you direct it to do  but it can also respond to your facial expressions and emotional
  and what better way to do this than by interpreting the signals naturally produced by our brain our center for control and experience 
 well it sounds like a pretty good idea but this task  as bruno mentioned isn 't an easy one for two main reasons 
 first the detection algorithms  
 our brain is made up of billions of active neurons around one hundred and seventy thousand km of combined axon length when these
  neurons interact the chemical reaction emits an electrical impulse which can be measured the majority of our functional brain
 is distributed over the
 outer surface layer of the brain and to increase the area that 's available for mental capacity the brain surface is highly folded 
 now this cortical folding presents a significant challenge for interpreting surface electrical impulses
 each individual 's cortex is folded differently very much like a fingerprint  
 so even though a signal may come from the same functional part of the brain 
 by the time the structure has been folded its physical location is very different between individuals even identical twins 
 there is no longer any consistency in the surface signals our breakthrough
 was to create an algorithm that unfolds the cortex so that we can map the signals closer to its source  and therefore making it capable of working across a mass population 
 the second challenge
 is the actual device for observing brainwaves  eeg measurements typically involve a hairnet with an array of sensors like the one that you can see here in the photo
  technician will put the electrodes onto the scalp using a conductive gel or paste
 and usually after a procedure of preparing the scalp by light abrasion now this is quite time consuming and isn 't the most comfortable process  
 and on top of that these systems actually cost in the tens of thousands of dollars  
  like to invite onstage
 evan grant who is one of last year 's speakers who 's kindly agreed to help me to demonstrate what we 've been able to develop 
 so the device that you see is 
  acquisition system it doesn 't require
  any scalp preparation no conductive gel or paste it only takes a few minutes to put on and for the signals to settle 
 it 's also wireless  so it gives you the freedom to move around and
 compared to the tens of thousands of dollars
 for a traditional eeg system this headset only costs a few hundred dollars now on to the detection
  so facial expressions as i mentioned before in emotional experiences
 are actually designed to work out of the box with some sensitivity adjustments available for personalization 
 but with the limited time we have available i 'd like to show you the cognitive suite  which is
 the ability for you to basically move virtual objects with your mind  now evan is new to this system so what we have to do first is
 create a new profile for him he 's obviously not joanne so
  add user evan
 okay  so the first thing we need to do with the cognitive suite is to start with training a neutral signal 
 with neutral there 's nothing in particular that evan needs to do he just hangs out he 's relaxed and the idea is to establish a baseline or normal state for his
  because every brain is different it takes eight seconds to do this and now that that 's done we can choose a 
  so evan choose something that you can visualize clearly in your mind evan grant let 's do
 pull tan le okay  so let 's choose pull 
 so the idea here now is that evan needs to imagine the object coming forward into the screen 
 and there 's a progress bar that will scroll across the screen while he 's doing that 
 the first time
  nothing will happen because the system has no idea how he thinks about pull but maintain that thought for the entire duration of the eight seconds so one
  so we have a little bit of time available so i 'm going to ask evan to do a really difficult task  and this one is difficult because
  it 's all about being able to visualize something that doesn 't exist in our physical world
 this is disappear so what you want to do at least with  movement based actions we do that all the time so you can visualize it but with disappear there 's really no analogies
 so evan what you want to do here is to imagine the cube slowly fading out okay  same sort of drill so one
  actually works even though you can only hold it for a little bit of time as i said it 's a very difficult
 process to imagine this and the great thing about it is that we 've only given the software one instance of how he thinks about disappear
  as there is a machine learning algorithm
  thank you evan you 're a wonderful wonderful
 example of the technology 
 so as you can see before there is a leveling system built into this software so that as evan or any user becomes more
  with the system they can continue to add more and more detections so that the system begins to differentiate between different distinct thoughts
  and once you 've trained up the detections these thoughts can be assigned or mapped to any computing platform  application or device 
 so i 'd like to show you a few examples because there are many possible applications for this new interface 
 in games and virtual worlds for example your facial expressions can naturally and intuitively be
  used to control an avatar or virtual character  
 obviously you can experience the fantasy of magic and control the world with your mind  
 and also colors  lighting sound and effects can dynamically respond to your emotional state to heighten the experience that
  in real time  and moving on to some applications developed by developers and researchers around the world with robots and simple machines for example
 in this case flying a toy helicopter simply by thinking lift with your mind 
 the technology can also be applied to real world applications in this example a smart home you know from the user interface of the control system
 to opening
 curtains or closing curtains
 and of course also to the lighting
 turning them on
 or off 
 and finally to real life changing applications such as being able to control an electric wheelchair in this example
  expressions are mapped to the movement commands man now blink right to go right now blink left
 to turn back 
  now smile to go straight
