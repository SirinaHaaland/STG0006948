StuartRussell_2017 1 StuartRussell_2017 15.13 22.97 <NA> <unk> one of the world 's greatest go players and he 's having what my friends in silicon valley call a holy cow moment 
StuartRussell_2017 1 StuartRussell_2017 23.83 28.98 <NA> a moment where we realize that ai is actually progressing a lot faster than we expected
StuartRussell_2017 1 StuartRussell_2017 30.02 37.74 <NA> <unk> so humans have lost on the go board what about the real world well <unk> the real world is much bigger much more complicated than the go board it
StuartRussell_2017 1 StuartRussell_2017 37.28 41.52 <NA> 's a lot less visible <unk> but it 's still a decision problem <unk> 
StuartRussell_2017 1 StuartRussell_2017 42.35 47.37 <NA> and if we think about some of the technologies that are coming down the pike
StuartRussell_2017 1 StuartRussell_2017 47.36 54.64 <NA> noriko [ arai ] mentioned that reading is not yet happening in machines at least with understanding <unk> but that will happen
StuartRussell_2017 1 StuartRussell_2017 54.93 59.95 <NA> <unk> and when that happens very soon afterwards machines will have read
StuartRussell_2017 1 StuartRussell_2017 59.58 62.83 <NA> everything that the human race has ever written <unk> 
StuartRussell_2017 1 StuartRussell_2017 63.27 70.45 <NA> and that will enable machines along with the ability to look further ahead than humans can as we 've already seen in go 
StuartRussell_2017 1 StuartRussell_2017 70.02 76.99 <NA> if they also have access to more information they 'll be able to make better decisions in the real world than we can 
StuartRussell_2017 1 StuartRussell_2017 78.15 79.94 <NA> so is that a good thing
StuartRussell_2017 1 StuartRussell_2017 80.01 84.38 <NA> <unk> well <unk> i hope so 
StuartRussell_2017 1 StuartRussell_2017 86.11 91.74 <NA> our entire civilization everything that we value is based on our intelligence <unk> 
StuartRussell_2017 1 StuartRussell_2017 91.74 95.63 <NA> and if we had access to a lot more intelligence <unk> 
StuartRussell_2017 1 StuartRussell_2017 95.2 99.47 <NA> then there 's really no limit to what the human race can do <unk> 
StuartRussell_2017 1 StuartRussell_2017 100.55 106.08 <NA> <unk> and i think this could be as some people have described it the biggest event in human history <unk> 
StuartRussell_2017 1 StuartRussell_2017 108.05 114.6 <NA> so why are people saying things like this that ai might spell the end of the human race 
StuartRussell_2017 1 StuartRussell_2017 114.86 121.47 <NA> is this a new thing <unk> is it just elon musk and bill gates and stephen hawking 
StuartRussell_2017 1 StuartRussell_2017 121.76 125.11 <NA> actually no this idea has been around for a while
StuartRussell_2017 1 StuartRussell_2017 127.14 131.2 <NA> <unk> even if we could keep the machines in a subservient position 
StuartRussell_2017 1 StuartRussell_2017 130.95 140.65 <NA> for instance by turning off the power at strategic moments and i 'll come back to that turning off the power idea later on we should as a species feel greatly humbled 
StuartRussell_2017 1 StuartRussell_2017 141.54 144.28 <NA> so who said this this is alan turing in <unk>
StuartRussell_2017 1 StuartRussell_2017 145.71 154.82 <NA> alan turing as you know is the father of computer science and in many ways the father of ai as well <unk> so if we think about this problem
StuartRussell_2017 1 StuartRussell_2017 154.96 161.42 <NA> <unk> problem of creating something more intelligent than your own species we might call this the gorilla problem 
StuartRussell_2017 1 StuartRussell_2017 161.65 168.32 <NA> because gorillas ' ancestors did this a few million years ago <unk> and now we can ask the gorillas 
StuartRussell_2017 1 StuartRussell_2017 168.16 174.57 <NA> was this a good idea <unk> so here they are having a meeting to discuss whether it was a good idea and
StuartRussell_2017 1 StuartRussell_2017 174.38 179.85 <NA> after a little while they conclude no this was a terrible idea our species is in dire straits
StuartRussell_2017 1 StuartRussell_2017 179.87 188.53 <NA> in fact you can see the existential sadness in their eyes so this queasy feeling that
StuartRussell_2017 1 StuartRussell_2017 188.53 193.47 <NA> making something smarter than your own species is is maybe not a good idea
StuartRussell_2017 1 StuartRussell_2017 193.88 200.64 <NA> what can we do about that <unk> well really nothing <unk> except stop doing ai
StuartRussell_2017 1 StuartRussell_2017 200.72 204.94 <NA> and because of all the benefits that i mentioned and because i 'm an ai researcher i 'm not
StuartRussell_2017 1 StuartRussell_2017 204.9 206.35 <NA> <unk> having that 
StuartRussell_2017 1 StuartRussell_2017 206.76 209.65 <NA> i actually want to be able to keep doing ai 
StuartRussell_2017 1 StuartRussell_2017 210.03 218.14 <NA> so we actually need to nail down the problem a bit more what exactly is the problem why is better ai possibly a catastrophe <unk> 
StuartRussell_2017 1 StuartRussell_2017 218.76 221.2 <NA> so here 's another quotation 
StuartRussell_2017 1 StuartRussell_2017 221.34 229.94 <NA> we had better be quite sure that the purpose put into the machine is the purpose which we really desire this was said by norbert wiener
StuartRussell_2017 1 StuartRussell_2017 230.01 238.64 <NA> in one thousand nine hundred and sixty shortly after he watched one of the very early learning systems learn to play checkers better than its creator <unk> 
StuartRussell_2017 1 StuartRussell_2017 240.04 244.7 <NA> but this could equally have been said by king midas
StuartRussell_2017 1 StuartRussell_2017 244.36 254.19 <NA> king midas said i want everything i touch to turn to gold and he got exactly what he asked for that was the purpose that he put into the machine <unk> so to speak
StuartRussell_2017 1 StuartRussell_2017 255.02 260.85 <NA> <unk> then his food and his drink and his relatives turned to gold and he died in misery and starvation 
StuartRussell_2017 1 StuartRussell_2017 261.83 264.52 <NA> so we 'll call this the king midas problem 
StuartRussell_2017 1 StuartRussell_2017 264.52 273.58 <NA> of stating an objective which is not in fact <unk> truly aligned with what we want in modern terms we call this the value alignment problem
StuartRussell_2017 1 StuartRussell_2017 277.08 286.63 <NA> <unk> putting in the wrong objective is not the only part of the problem there 's another part if you put an objective into a machine even something as simple as fetch the coffee 
StuartRussell_2017 1 StuartRussell_2017 287.31 289.93 <NA> the machine says to itself 
StuartRussell_2017 1 StuartRussell_2017 290.13 295.18 <NA> well how might i fail to fetch the coffee <unk> someone might switch me off <unk> 
StuartRussell_2017 1 StuartRussell_2017 294.96 304.85 <NA> ok i have to take steps to prevent that i will disable my ' off ' switch <unk> i will do anything to defend myself against interference with this objective that i have been given
StuartRussell_2017 1 StuartRussell_2017 305.53 308.54 <NA> so this <unk> single minded pursuit
StuartRussell_2017 1 StuartRussell_2017 308.62 315.32 <NA> in a very defensive mode of an objective that is in fact <unk> not aligned with the true objectives of the human race
StuartRussell_2017 1 StuartRussell_2017 315.52 321.5 <NA> that 's the problem that we face <unk> and in fact that 's the <unk>
StuartRussell_2017 1 StuartRussell_2017 322.51 327.93 <NA> <unk> from this talk <unk> if you want to remember one thing <unk> it 's that you can 't fetch the coffee if you 're dead
StuartRussell_2017 1 StuartRussell_2017 329.93 333.54 <NA> <unk> simple just remember that repeat it to yourself three times a day 
StuartRussell_2017 1 StuartRussell_2017 334.85 337.95 <NA> and in fact this is exactly the plot
StuartRussell_2017 1 StuartRussell_2017 337.52 339.24 <NA> of 
StuartRussell_2017 1 StuartRussell_2017 339.23 348.96 <NA> two thousand and one [ a space odyssey ] hal has an objective a mission which is not aligned with the objectives of the humans and that leads to this conflict 
StuartRussell_2017 1 StuartRussell_2017 348.86 350.02 <NA> now fortunately
StuartRussell_2017 1 StuartRussell_2017 352.26 358.12 <NA> <unk> he 's pretty smart <unk> but eventually dave outwits him and manages to switch him off <unk> 
StuartRussell_2017 1 StuartRussell_2017 361.23 363.67 <NA> but we might not be so lucky <unk> 
StuartRussell_2017 1 StuartRussell_2017 367.56 369.76 <NA> so what are we going to do 
StuartRussell_2017 1 StuartRussell_2017 371.76 375.08 <NA> i 'm trying to redefine ai to
StuartRussell_2017 1 StuartRussell_2017 375 379.28 <NA> <unk> get away from this classical notion of machines that
StuartRussell_2017 1 StuartRussell_2017 378.85 385.34 <NA> intelligently pursue objectives there are three principles involved the first one
StuartRussell_2017 1 StuartRussell_2017 385 391.01 <NA> is a principle of altruism if you like that the robot 's only objective
StuartRussell_2017 1 StuartRussell_2017 390.55 398.15 <NA> is to maximize the realization of human objectives of human values and by values here i don 't mean <unk>
StuartRussell_2017 1 StuartRussell_2017 399.28 404.94 <NA> <unk> values i just mean whatever it is that the human would prefer their life to be like
StuartRussell_2017 1 StuartRussell_2017 406.76 415.8 <NA> and so this actually violates asimov 's law that the robot has to protect its own existence it has no interest in preserving its existence whatsoever <unk> 
StuartRussell_2017 1 StuartRussell_2017 416.81 424.96 <NA> the second law is a law of humility if you like <unk> and this turns out to be really important to make robots safe <unk> 
StuartRussell_2017 1 StuartRussell_2017 425.64 434.32 <NA> <unk> it says that the robot does not know what those human values are so it has to maximize them <unk> but it doesn 't know what they are 
StuartRussell_2017 1 StuartRussell_2017 434.64 441.13 <NA> and that avoids this problem of single minded pursuit of an objective <unk> this uncertainty turns out to be crucial <unk> 
StuartRussell_2017 1 StuartRussell_2017 441.12 446.32 <NA> now in order to be useful to us it has to have some idea of what we want 
StuartRussell_2017 1 StuartRussell_2017 446.67 454.94 <NA> it obtains that information primarily by observation of human choices so our own choices reveal information
StuartRussell_2017 1 StuartRussell_2017 455.07 458.87 <NA> about what it is that we prefer our lives to be like <unk> 
StuartRussell_2017 1 StuartRussell_2017 460 467.48 <NA> so those are the three principles let 's see how that applies to this question of can you switch the machine off as turing suggested <unk> 
StuartRussell_2017 1 StuartRussell_2017 468.46 474.9 <NA> so here 's a pr2 robot this is one that we have in our lab and it has a big red off switch
StuartRussell_2017 1 StuartRussell_2017 476.87 485.82 <NA> <unk> is it going to let you switch it off <unk> if we do it the classical way we give it the objective of fetch the coffee i must fetch the coffee <unk> i can 't fetch the coffee
StuartRussell_2017 1 StuartRussell_2017 485.65 494.37 <NA> if i 'm dead so obviously the pr2 has been listening to my talk <unk> and so it says therefore i must disable my ' off ' switch 
StuartRussell_2017 1 StuartRussell_2017 494.39 499.29 <NA> and probably taser all the other people in starbucks who might interfere with me 
StuartRussell_2017 1 StuartRussell_2017 501.81 509.53 <NA> <unk> this seems to be inevitable right this kind of of failure mode seems to be inevitable and it follows from having a concrete definite objective <unk> 
StuartRussell_2017 1 StuartRussell_2017 510.21 518.92 <NA> so what happens if the machine is uncertain about the objective well it reasons in a different way it says ok the human might switch me off but
StuartRussell_2017 1 StuartRussell_2017 518.58 528.32 <NA> only if i 'm doing something wrong well i don 't really know what wrong is but i know that i don 't want to do it so that 's the first and second principles right there
StuartRussell_2017 1 StuartRussell_2017 530.04 532.85 <NA> i should let the human switch me off 
StuartRussell_2017 1 StuartRussell_2017 533.14 539.91 <NA> and in fact you can calculate the incentive that the robot has to allow the human to switch it off <unk> 
StuartRussell_2017 1 StuartRussell_2017 539.91 544.97 <NA> and it 's directly tied to the degree of uncertainty about the underlying objective <unk> 
StuartRussell_2017 1 StuartRussell_2017 545.41 547.59 <NA> and then when the machine
StuartRussell_2017 1 StuartRussell_2017 547.4 549.96 <NA> is switched off <unk> that third principle comes
StuartRussell_2017 1 StuartRussell_2017 550.31 555.48 <NA> <unk> it learns something about the objectives it should be pursuing because it learns that what it did wasn 't right
StuartRussell_2017 1 StuartRussell_2017 556.01 563.83 <NA> in fact we can with suitable use of greek symbols as mathematicians usually do <unk> we can actually prove a theorem
StuartRussell_2017 1 StuartRussell_2017 563.59 572.79 <NA> that says that such a robot is provably beneficial to the human you are provably better off with a machine that 's designed in this way than without it 
StuartRussell_2017 1 StuartRussell_2017 572.63 578.29 <NA> so this is a very simple example but this is the first step in in what we 're trying to do with
StuartRussell_2017 1 StuartRussell_2017 582.06 583.72 <NA> now 
StuartRussell_2017 1 StuartRussell_2017 583.44 590.35 <NA> this third principle i think is the one that you 're probably scratching your head over you 're probably thinking well you know <unk> 
StuartRussell_2017 1 StuartRussell_2017 590.31 593.38 <NA> i behave badly <unk> i don 't want my
StuartRussell_2017 1 StuartRussell_2017 592.95 602.17 <NA> my robot to behave like me i sneak down in the middle of the night and take stuff from the fridge i do this and that there 's all kinds of things you don 't want the robot doing
StuartRussell_2017 1 StuartRussell_2017 602.16 604.88 <NA> <unk> but in fact it doesn 't quite work that way just because you
StuartRussell_2017 1 StuartRussell_2017 604.81 606.44 <NA> <unk> behave badly
StuartRussell_2017 1 StuartRussell_2017 606.46 611.12 <NA> doesn 't mean the robot is going to copy your behavior it 's going to understand your motivations
StuartRussell_2017 1 StuartRussell_2017 610.89 614.72 <NA> and maybe help you resist them <unk> if appropriate <unk> 
StuartRussell_2017 1 StuartRussell_2017 615.61 624.75 <NA> but it 's still difficult what we 're trying to do in fact <unk> is to allow machines to predict for any person
StuartRussell_2017 1 StuartRussell_2017 624.59 629.07 <NA> and for any possible life that they could live <unk> and the lives of everybody else
StuartRussell_2017 1 StuartRussell_2017 629.87 632.07 <NA> <unk> which would they prefer 
StuartRussell_2017 1 StuartRussell_2017 633.44 639.21 <NA> and there are many many difficulties involved in doing this i don 't expect that this is going to get solved very quickly <unk>
StuartRussell_2017 1 StuartRussell_2017 639.2 642.93 <NA> the real difficulties in fact <unk> are us <unk> 
StuartRussell_2017 1 StuartRussell_2017 643.58 649.74 <NA> as i have already mentioned <unk> we behave badly in fact some of us are downright nasty
StuartRussell_2017 1 StuartRussell_2017 650.64 658.33 <NA> <unk> now the robot as i said doesn 't have to copy the behavior <unk> the robot does not have any objective of its own it 's purely altruistic <unk> 
StuartRussell_2017 1 StuartRussell_2017 658.74 667.84 <NA> and it 's not designed just to satisfy the desires of one person the user <unk> but in fact it has to respect the preferences of everybody <unk> 
StuartRussell_2017 1 StuartRussell_2017 668.61 674.66 <NA> so it can deal with a certain amount of nastiness and it can even understand that your nastiness for
StuartRussell_2017 1 StuartRussell_2017 675.43 678.14 <NA> <unk> you may take bribes as a passport official
StuartRussell_2017 1 StuartRussell_2017 677.91 681.95 <NA> because you need to feed your family and send your kids to school <unk>
StuartRussell_2017 1 StuartRussell_2017 681.46 687.08 <NA> it can understand that it doesn 't mean it 's going to steal <unk> in fact it 'll just help you send your kids to school 
StuartRussell_2017 1 StuartRussell_2017 688.39 691.86 <NA> we are also computationally limited
StuartRussell_2017 1 StuartRussell_2017 693.28 702.5 <NA> <unk> a brilliant go player but he still lost so if we look at his actions he took an action that lost the game <unk> that doesn 't mean he wanted to lose 
StuartRussell_2017 1 StuartRussell_2017 702.7 704.94 <NA> so to understand his behavior
StuartRussell_2017 1 StuartRussell_2017 705.19 713.6 <NA> <unk> we actually have to invert through a model of human cognition that includes our computational limitations a very complicated model <unk> 
StuartRussell_2017 1 StuartRussell_2017 713.61 716.73 <NA> but it 's still something that we can work on understanding <unk> 
StuartRussell_2017 1 StuartRussell_2017 717.26 724.66 <NA> probably the most difficult part from my point of view as an ai researcher is the fact that there are lots of us <unk>
StuartRussell_2017 1 StuartRussell_2017 726.15 731.65 <NA> <unk> and so the machine has to somehow trade off weigh up the preferences of many different people 
StuartRussell_2017 1 StuartRussell_2017 731.58 743.29 <NA> and there are different ways to do that <unk> economists sociologists <unk> moral philosophers have understood that and we are actively looking for collaboration let 's have a look and see what happens when you get that wrong 
StuartRussell_2017 1 StuartRussell_2017 742.89 748.76 <NA> so you can have a conversation for example with your intelligent personal assistant that might be available
StuartRussell_2017 1 StuartRussell_2017 748.55 749.99 <NA> in a few years ' time think
StuartRussell_2017 1 StuartRussell_2017 754.3 761.49 <NA> <unk> your wife called to remind you about dinner tonight and of course you 've forgotten what what dinner <unk> what are you talking about
StuartRussell_2017 1 StuartRussell_2017 779.99 788.07 <NA> <unk> well what am i going to do i can 't just tell him i 'm too busy don 't worry <unk> i arranged for his plane to be delayed 
StuartRussell_2017 1 StuartRussell_2017 789.65 795.39 <NA> some kind of computer malfunction really you can do that 
StuartRussell_2017 1 StuartRussell_2017 795.83 800.5 <NA> he sends his profound apologies and looks forward to meeting you for lunch tomorrow
StuartRussell_2017 1 StuartRussell_2017 804.93 811.93 <NA> <unk> 's a slight mistake going on <unk> this is clearly following my wife 's values which is happy wife happy life 
StuartRussell_2017 1 StuartRussell_2017 813.06 817.7 <NA> it could go the other way <unk> you could come home after a hard day 's work
StuartRussell_2017 1 StuartRussell_2017 832.06 840.38 <NA> <unk> there are humans in south sudan who are in more urgent need than you so i 'm leaving make your own dinner 
StuartRussell_2017 1 StuartRussell_2017 842.17 846.41 <NA> so we have to solve these problems <unk> and i 'm looking forward to working on them <unk> 
StuartRussell_2017 1 StuartRussell_2017 846.52 854.88 <NA> there are reasons for optimism one reason is there is a massive amount of data because remember i said they 're going to read everything the human race has ever written
StuartRussell_2017 1 StuartRussell_2017 855.05 858.41 <NA> <unk> most of what we write about is human beings doing things
StuartRussell_2017 1 StuartRussell_2017 858.42 866.13 <NA> and other people getting upset about it so there 's a massive amount of data to learn from there 's also a very strong economic incentive
StuartRussell_2017 1 StuartRussell_2017 866.72 871.74 <NA> to get this right <unk> so imagine your domestic robot 's at home you 're late from work again
StuartRussell_2017 1 StuartRussell_2017 871.74 877.68 <NA> and the robot has to feed the kids and the kids are hungry and there 's nothing in the fridge <unk> and the robot
StuartRussell_2017 1 StuartRussell_2017 877.52 878.83 <NA> sees the cat
StuartRussell_2017 1 StuartRussell_2017 880.71 890.92 <NA> <unk> and the robot hasn 't quite learned the human value function properly so it doesn 't understand the sentimental value of the cat outweighs the nutritional value of the cat <unk> 
StuartRussell_2017 1 StuartRussell_2017 890.64 894.91 <NA> so then what happens well <unk> <unk> 
StuartRussell_2017 1 StuartRussell_2017 895.17 900.17 <NA> it happens like this deranged robot cooks kitty for family dinner
StuartRussell_2017 1 StuartRussell_2017 901.09 907.91 <NA> <unk> that one incident would be the end of the domestic robot industry <unk> so there 's a huge incentive to get this right
StuartRussell_2017 1 StuartRussell_2017 907.72 911.3 <NA> long before we reach superintelligent machines <unk> 
StuartRussell_2017 1 StuartRussell_2017 911.53 913.37 <NA> so to summarize 
StuartRussell_2017 1 StuartRussell_2017 913.37 924.98 <NA> i 'm actually trying to change the definition of ai so that we have provably beneficial machines <unk> and the principles are machines that are altruistic <unk> that want to achieve only our objectives <unk> 
StuartRussell_2017 1 StuartRussell_2017 924.52 929.94 <NA> but that are uncertain about what those objectives are and will watch all of us
StuartRussell_2017 1 StuartRussell_2017 929.95 933.69 <NA> to learn more about what it is that we really want 
StuartRussell_2017 1 StuartRussell_2017 933.74 939.15 <NA> and hopefully in the process we will learn to be better people <unk> thank you very much chris anderson so interesting 
StuartRussell_2017 1 StuartRussell_2017 943.04 950.29 <NA> stuart we 're going to stand here a bit because i think they 're setting up for our next speaker a couple of questions
StuartRussell_2017 1 StuartRussell_2017 950.97 956.65 <NA> <unk> the idea of programming in ignorance seems intuitively really powerful <unk> as you get to <unk>
StuartRussell_2017 1 StuartRussell_2017 957.62 967.43 <NA> what 's going to stop a robot reading literature and discovering this idea that knowledge is actually better than ignorance and still just shifting its own goals and rewriting that
StuartRussell_2017 1 StuartRussell_2017 979.96 981.39 <NA> <unk> as it becomes more
StuartRussell_2017 1 StuartRussell_2017 981.39 990.41 <NA> correct so the the evidence is there and <unk> and it 's going to be designed to interpret it correctly it will understand for example that books
StuartRussell_2017 1 StuartRussell_2017 990.1 1000.11 <NA> are very biased in the in the evidence they contain they only talk about kings and princes and elite white male people doing stuff <unk> so it 's a complicated problem
StuartRussell_2017 1 StuartRussell_2017 1002.26 1008.97 <NA> <unk> it learns more about our objectives it will become more and more useful to us ca and you couldn 't just boil it down to one law you know <unk> 
StuartRussell_2017 1 StuartRussell_2017 1008.73 1015.74 <NA> hardwired in if any human ever tries to switch me off <unk> i comply i comply sr 
StuartRussell_2017 1 StuartRussell_2017 1015.74 1021.33 <NA> absolutely not <unk> that would be a terrible idea so imagine that you have a self driving car
StuartRussell_2017 1 StuartRussell_2017 1021.09 1024.99 <NA> and you want to send your five year old off to preschool
StuartRussell_2017 1 StuartRussell_2017 1026.24 1031.57 <NA> <unk> to be able to switch off the car while it 's driving along probably not so it needs to understand
StuartRussell_2017 1 StuartRussell_2017 1031.39 1044.31 <NA> how rational and sensible the person is the more rational the person <unk> the more willing you are to be switched off if the person is completely random or even malicious then you 're less willing to be switched off ca all
StuartRussell_2017 1 StuartRussell_2017 1044.3 1048.73 <NA> right stuart can i just say i really really hope you figure this out for us thank you so much for that
