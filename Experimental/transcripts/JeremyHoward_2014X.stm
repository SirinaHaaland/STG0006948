JeremyHoward_2014X 1 JeremyHoward_2014X 13.21 23.51 <NA> <unk> it used to be that if you wanted to get a computer to do something new you would have to program it now programming for those of you here that haven 't done it yourself <unk> requires laying out <unk>
JeremyHoward_2014X 1 JeremyHoward_2014X 23.41 29.97 <NA> <unk> in excruciating detail every single step that you want the computer to do in order to achieve your goal
JeremyHoward_2014X 1 JeremyHoward_2014X 29.78 31.02 <NA> <unk> goal 
JeremyHoward_2014X 1 JeremyHoward_2014X 30.92 40.68 <NA> now if you want to do something that you don 't know how to do yourself then this is going to be a great challenge <unk> so this was the challenge faced by this man arthur samuel in one thousand nine <unk>
JeremyHoward_2014X 1 JeremyHoward_2014X 41.6 50.26 <NA> <unk> he wanted to get this computer to be able to beat him at checkers <unk> how can you write a program lay out in excruciating detail
JeremyHoward_2014X 1 JeremyHoward_2014X 50.22 60.82 <NA> <unk> how to be better than you at checkers <unk> so he came up with an idea he had the computer play against itself thousands of times and learn how to play checkers <unk> 
JeremyHoward_2014X 1 JeremyHoward_2014X 60.54 66.49 <NA> and indeed it worked <unk> and in fact by one thousand nine hundred and sixty two this computer had beaten the connecticut state champion 
JeremyHoward_2014X 1 JeremyHoward_2014X 66.84 74.47 <NA> so arthur samuel was the father of machine learning and i have a great debt to him because i am a machine learning practitioner
JeremyHoward_2014X 1 JeremyHoward_2014X 74.46 78.86 <NA> i was the president of kaggle a community of over two hundred thousand machine learning
JeremyHoward_2014X 1 JeremyHoward_2014X 80.22 89 <NA> kaggle puts up competitions to try and get them to solve previously unsolved problems <unk> and it 's been successful hundreds of times <unk> 
JeremyHoward_2014X 1 JeremyHoward_2014X 88.99 97.82 <NA> so from this vantage point i was able to find out a lot about what machine learning can do in the past can do today <unk> and what it could do in the future <unk> 
JeremyHoward_2014X 1 JeremyHoward_2014X 97.75 104.76 <NA> perhaps the first big success of machine learning commercially was google <unk> google showed that it is possible to find
JeremyHoward_2014X 1 JeremyHoward_2014X 104.69 105.9 <NA> <unk> information
JeremyHoward_2014X 1 JeremyHoward_2014X 105.7 110.61 <NA> by using a computer algorithm and this algorithm is based on machine learning <unk> 
JeremyHoward_2014X 1 JeremyHoward_2014X 110.21 120.48 <NA> since that time there have been many commercial successes of machine learning <unk> companies like amazon and netflix use machine learning to suggest products that you might like to buy <unk> movies that you <unk>
JeremyHoward_2014X 1 JeremyHoward_2014X 120.38 129.94 <NA> <unk> might like to watch sometimes it 's almost creepy <unk> companies like linkedin and facebook sometimes will tell you about who your friends might be and you have no idea how it did it
JeremyHoward_2014X 1 JeremyHoward_2014X 129.95 139.24 <NA> and this is because it 's using the power of machine learning <unk> these are algorithms that have learned how to do this from data rather than being programmed by hand <unk> 
JeremyHoward_2014X 1 JeremyHoward_2014X 139.05 145.66 <NA> this is also how ibm was successful in getting watson to beat the two world champions at jeopardy 
JeremyHoward_2014X 1 JeremyHoward_2014X 145.65 149.33 <NA> answering incredibly subtle and complex questions like this one
JeremyHoward_2014X 1 JeremyHoward_2014X 154.69 164.9 <NA> <unk> cars <unk> if you want to be able to tell the difference between say a tree and a pedestrian well that 's pretty important <unk> we don 't know how to write those programs by hand <unk> but with machine learning this <unk>
JeremyHoward_2014X 1 JeremyHoward_2014X 164.8 171.86 <NA> <unk> is now possible and in fact this car has driven over a million miles without any accidents on regular roads
JeremyHoward_2014X 1 JeremyHoward_2014X 175.19 183.48 <NA> <unk> can learn and computers can learn to do things that we actually sometimes don 't know how to do ourselves or maybe can do them better than us 
JeremyHoward_2014X 1 JeremyHoward_2014X 183.41 192.57 <NA> one of the most amazing examples i 've seen of machine learning happened on a project that i ran at kaggle where a team run
JeremyHoward_2014X 1 JeremyHoward_2014X 192.2 198.13 <NA> by a guy called geoffrey hinton from the university of toronto won a competition for automatic drug discovery 
JeremyHoward_2014X 1 JeremyHoward_2014X 198.13 204.79 <NA> now what was extraordinary here is not just that they beat all of the algorithms developed by merck or the international academic community
JeremyHoward_2014X 1 JeremyHoward_2014X 204.99 214.21 <NA> <unk> but nobody on the team had any background in chemistry or biology or life sciences and they did it in two weeks how did they do this 
JeremyHoward_2014X 1 JeremyHoward_2014X 213.9 217.22 <NA> they used an extraordinary algorithm called deep learning
JeremyHoward_2014X 1 JeremyHoward_2014X 217.22 224.93 <NA> so important was this that in fact the success was covered in the new york times in a front page article a few weeks later <unk> this is geoffrey hinton here on the
JeremyHoward_2014X 1 JeremyHoward_2014X 227.05 235.7 <NA> <unk> is an algorithm inspired by how the human brain works <unk> and as a result it 's an algorithm which has no theoretical limitations on what it can do 
JeremyHoward_2014X 1 JeremyHoward_2014X 235.7 240.36 <NA> the more data you give it and the more computation time you give it the better it gets 
JeremyHoward_2014X 1 JeremyHoward_2014X 240.14 246.53 <NA> the new york times also showed in this article another extraordinary result of deep learning which i 'm going to show you now 
JeremyHoward_2014X 1 JeremyHoward_2014X 247.48 249.9 <NA> it shows that computers can
JeremyHoward_2014X 1 JeremyHoward_2014X 256.46 261.42 <NA> <unk> take in this process is to actually speak to you in chinese 
JeremyHoward_2014X 1 JeremyHoward_2014X 263.24 270.52 <NA> now the key thing there is we 've been able to take a large amount of information from many chinese speakers
JeremyHoward_2014X 1 JeremyHoward_2014X 270.52 277.51 <NA> and produce a text to speech system that takes chinese text and converts it into chinese language
JeremyHoward_2014X 1 JeremyHoward_2014X 279.84 287.54 <NA> <unk> hour or so of my own voice and we 've used that to modulate the standard text to speech system so that it would sound like me
JeremyHoward_2014X 1 JeremyHoward_2014X 313.57 316.64 <NA> <unk> that was at a machine learning conference in china
JeremyHoward_2014X 1 JeremyHoward_2014X 316.21 324.23 <NA> <unk> it 's not often actually at academic conferences that you do hear spontaneous applause <unk> although of course sometimes at tedx conferences feel free
JeremyHoward_2014X 1 JeremyHoward_2014X 324.01 327.21 <NA> <unk> everything you saw there was happening with deep learning
JeremyHoward_2014X 1 JeremyHoward_2014X 329.78 337.53 <NA> <unk> english was deep learning <unk> the translation to chinese and the text in the top right deep learning and the construction of the voice was deep learning as well
JeremyHoward_2014X 1 JeremyHoward_2014X 338.09 348.6 <NA> so deep learning is this extraordinary thing it 's a single algorithm that can seem to do almost anything <unk> and i discovered that a year earlier it had also learned to see in this obscure <unk>
JeremyHoward_2014X 1 JeremyHoward_2014X 348.5 354.97 <NA> <unk> competition from germany called the german traffic sign recognition benchmark deep learning had learned to recognize traffic signs like this
JeremyHoward_2014X 1 JeremyHoward_2014X 355.41 369.67 <NA> not only could it recognize the traffic signs better than any other algorithm the leaderboard actually showed it was better than people about twice as good as people <unk> so by two thousand and eleven we had the first example of computers that can see better than people <unk> 
JeremyHoward_2014X 1 JeremyHoward_2014X 369.24 379.91 <NA> since that time a lot has happened in two thousand and twelve google announced that they had a deep learning algorithm watch youtube videos and crunched the data on sixteen thousand computers for a month and
JeremyHoward_2014X 1 JeremyHoward_2014X 379.93 394.19 <NA> <unk> computer independently learned about concepts such as people and cats just by watching the videos <unk> this is much like the way that humans learn <unk> humans don 't learn by being told what they see but by learning for themselves what these things are 
JeremyHoward_2014X 1 JeremyHoward_2014X 394.09 400.02 <NA> also in two thousand and twelve geoffrey hinton who we saw earlier <unk> won the very popular <unk> imagenet
JeremyHoward_2014X 1 JeremyHoward_2014X 400.97 406 <NA> <unk> looking to try to figure out from one and a half million images what they 're pictures of 
JeremyHoward_2014X 1 JeremyHoward_2014X 405.76 416.16 <NA> as of two thousand and fourteen we 're now down to a six percent error rate in image recognition <unk> this is better than people <unk> again so machines really are doing an extraordinarily good <unk>
JeremyHoward_2014X 1 JeremyHoward_2014X 416.06 425.95 <NA> <unk> job of this and it is now being used in industry for example <unk> google announced last year that they had mapped every single location in france
JeremyHoward_2014X 1 JeremyHoward_2014X 429.81 439.78 <NA> <unk> images into a deep learning algorithm to recognize and read street numbers imagine how long it would have taken before <unk> dozens of people many years <unk> 
JeremyHoward_2014X 1 JeremyHoward_2014X 440.28 454.85 <NA> this is also happening in china baidu is kind of the chinese google i guess <unk> and what you see here in the top left is an example of a picture that i uploaded to baidu 's deep learning system and underneath you can see that the system has
JeremyHoward_2014X 1 JeremyHoward_2014X 454.78 464.7 <NA> <unk> understood what that picture is and found similar images the similar images actually have similar backgrounds similar directions of the faces <unk> even some with their tongue out 
JeremyHoward_2014X 1 JeremyHoward_2014X 464.49 469.2 <NA> this is not clearly looking at the text of a web page all i uploaded was an image <unk> 
JeremyHoward_2014X 1 JeremyHoward_2014X 469.2 474.96 <NA> so we now have computers which really understand what they see and can therefore search databases of
JeremyHoward_2014X 1 JeremyHoward_2014X 475 481.11 <NA> <unk> hundreds of millions of images in real time <unk> so what does it mean now that computers can see well
JeremyHoward_2014X 1 JeremyHoward_2014X 481.25 486 <NA> it 's not just that computers can see <unk> in fact deep learning has done more than that 
JeremyHoward_2014X 1 JeremyHoward_2014X 485.6 499.38 <NA> complex nuanced sentences like this one are now understandable with deep learning algorithms as you can see here this stanford based system showing the red dot at the top has figured out that this sentence is expressing negative sentiment 
JeremyHoward_2014X 1 JeremyHoward_2014X 500.1 507.31 <NA> <unk> now in fact is near human performance at understanding what sentences are about and what it is saying about those things 
JeremyHoward_2014X 1 JeremyHoward_2014X 508.02 514.03 <NA> also deep learning has been used to read chinese <unk> again at about native chinese speaker level
JeremyHoward_2014X 1 JeremyHoward_2014X 513.63 524.26 <NA> <unk> this algorithm developed out of switzerland by people <unk> none of whom speak or understand any chinese as i say <unk> using deep learning is about the best system in the world for this
JeremyHoward_2014X 1 JeremyHoward_2014X 524.04 528.04 <NA> <unk> even compared to native human understanding 
JeremyHoward_2014X 1 JeremyHoward_2014X 528.72 529.97 <NA> this is a system
JeremyHoward_2014X 1 JeremyHoward_2014X 529.9 540.62 <NA> <unk> that we put together at my company which shows putting all this stuff together these are pictures which have no text attached <unk> and as i 'm typing in here sentences in real time it 's understanding <unk>
JeremyHoward_2014X 1 JeremyHoward_2014X 540.52 551.27 <NA> <unk> these pictures and figuring out what they 're about and finding pictures that are similar to the text that i 'm writing <unk> so you can see it 's actually understanding my sentences and actually understanding these pictures 
JeremyHoward_2014X 1 JeremyHoward_2014X 554.9 568.32 <NA> <unk> in things and it will show you pictures <unk> but actually what it 's doing is it 's searching the webpage for the text this is very different from actually understanding the images <unk> this is something that computers have only been able to do for the first time in the last few months <unk> 
JeremyHoward_2014X 1 JeremyHoward_2014X 569 576.69 <NA> so we can see now that computers can not only see but they can also read and of course we 've shown that they can understand what they hear <unk> 
JeremyHoward_2014X 1 JeremyHoward_2014X 576.29 579.88 <NA> perhaps not surprising now that i 'm going to tell you they can write
JeremyHoward_2014X 1 JeremyHoward_2014X 579.87 584.38 <NA> here is some text that i generated using a deep learning algorithm yesterday <unk> 
JeremyHoward_2014X 1 JeremyHoward_2014X 585.51 594.88 <NA> and here is some text that an algorithm out of stanford generated each of these sentences was generated by a deep learning algorithm to describe each of those pictures 
JeremyHoward_2014X 1 JeremyHoward_2014X 594.88 603.14 <NA> this algorithm before has never seen a man in a black shirt playing a guitar <unk> it 's seen a man before it 's seen black before it 's seen a guitar before
JeremyHoward_2014X 1 JeremyHoward_2014X 605.08 613.41 <NA> <unk> this novel description of this picture we 're still not quite at human performance here but we 're close <unk> in tests <unk> humans prefer the
JeremyHoward_2014X 1 JeremyHoward_2014X 614.44 625.01 <NA> <unk> caption one out of four times now this system is now only two weeks old so probably within the next year the computer algorithm will be well past human performance at the rate things are going <unk> 
JeremyHoward_2014X 1 JeremyHoward_2014X 625.42 629.94 <NA> so computers can also write <unk> so we put all this together and it
JeremyHoward_2014X 1 JeremyHoward_2014X 629.9 636.42 <NA> <unk> leads to very exciting opportunities for example in medicine a team in boston announced that they had discovered
JeremyHoward_2014X 1 JeremyHoward_2014X 635.96 643.44 <NA> dozens of new clinically relevant features of tumors which help doctors make a prognosis of a cancer 
JeremyHoward_2014X 1 JeremyHoward_2014X 643.91 650.11 <NA> very similarly <unk> in stanford a group there announced that looking at tissues under magnification
JeremyHoward_2014X 1 JeremyHoward_2014X 650.34 659.32 <NA> <unk> they 've developed a machine learning based system which in fact is better than human pathologists at predicting survival rates for cancer sufferers <unk> 
JeremyHoward_2014X 1 JeremyHoward_2014X 658.89 665.23 <NA> in both of these cases not only were the predictions more accurate but they generated new insightful science 
JeremyHoward_2014X 1 JeremyHoward_2014X 665.23 671.75 <NA> in the radiology case they were new clinical indicators that humans can understand in this pathology case
JeremyHoward_2014X 1 JeremyHoward_2014X 672.84 675.17 <NA> <unk> the computer system actually discovered that the cells
JeremyHoward_2014X 1 JeremyHoward_2014X 676.69 686 <NA> <unk> as important as the cancer cells themselves in making a diagnosis <unk> this is the opposite of what pathologists had been taught for decades <unk> 
JeremyHoward_2014X 1 JeremyHoward_2014X 687.01 700.04 <NA> in each of those two cases they were systems developed by a combination of medical experts and machine learning experts but as of last year we 're now beyond that too <unk> this is an example of identifying cancerous areas
JeremyHoward_2014X 1 JeremyHoward_2014X 700.04 702.65 <NA> of human tissue under a microscope
JeremyHoward_2014X 1 JeremyHoward_2014X 702.28 704.97 <NA> <unk> the system being shown here can identify
JeremyHoward_2014X 1 JeremyHoward_2014X 705.11 715.92 <NA> <unk> those areas more accurately or about as accurately as human pathologists but was built entirely with deep learning using no medical expertise by people who have no background in the field
JeremyHoward_2014X 1 JeremyHoward_2014X 716.09 719.4 <NA> <unk> similarly here <unk> this neuron segmentation <unk> 
JeremyHoward_2014X 1 JeremyHoward_2014X 719.4 727.33 <NA> we can now segment neurons about as accurately as humans can but this system was developed with deep learning using people with no previous background in medicine
JeremyHoward_2014X 1 JeremyHoward_2014X 729.96 737.14 <NA> <unk> as somebody with no previous background in medicine i seem to be entirely well qualified to start a new medical company which i did 
JeremyHoward_2014X 1 JeremyHoward_2014X 737.88 744.88 <NA> i was kind of terrified of doing it but the theory seemed to suggest that it ought to be possible to do very useful medicine
JeremyHoward_2014X 1 JeremyHoward_2014X 744.88 754.19 <NA> using just these data analytic techniques <unk> and thankfully the feedback has been fantastic not just from the media but from the medical community who have been very supportive
JeremyHoward_2014X 1 JeremyHoward_2014X 755.11 765.2 <NA> <unk> the theory is that we can take the middle part of the medical process and turn that into data analysis as much as possible leaving doctors to do what they 're best at 
JeremyHoward_2014X 1 JeremyHoward_2014X 764.95 777.12 <NA> i want to give you an example <unk> it now takes us about fifteen minutes to generate a new medical diagnostic test and i 'll show you that in real time now but i 've compressed it down to three minutes by cutting some pieces out 
JeremyHoward_2014X 1 JeremyHoward_2014X 777.12 779.61 <NA> rather than showing you creating a medical
JeremyHoward_2014X 1 JeremyHoward_2014X 780.14 786.18 <NA> <unk> test i 'm going to show you a diagnostic test of car images <unk> because that 's something we can all understand <unk> 
JeremyHoward_2014X 1 JeremyHoward_2014X 785.84 794.62 <NA> so here we 're starting with about one point five million car images <unk> and i want to create something that can split them into the angle of the photo that 's being taken 
JeremyHoward_2014X 1 JeremyHoward_2014X 794.62 798.93 <NA> so these images are entirely unlabeled <unk> so i have to start from scratch 
JeremyHoward_2014X 1 JeremyHoward_2014X 798.59 799.93 <NA> with our deep learning
JeremyHoward_2014X 1 JeremyHoward_2014X 800.37 803.99 <NA> <unk> it can automatically identify areas of structure in these images <unk> 
JeremyHoward_2014X 1 JeremyHoward_2014X 803.99 817.78 <NA> so the nice thing is that the human and the computer can now work together <unk> so the human as you can see here is telling the computer about areas of interest which it it wants the computer then to try and use to improve its algorithm 
JeremyHoward_2014X 1 JeremyHoward_2014X 817.53 820 <NA> now these deep learning systems actually are in <unk>
JeremyHoward_2014X 1 JeremyHoward_2014X 821.25 829.97 <NA> <unk> space so you can see here the computer rotating this through that space trying to find new areas of structure <unk> and when it does so successfully the human who is driving
JeremyHoward_2014X 1 JeremyHoward_2014X 830.11 838.07 <NA> <unk> it can then point out the areas that are interesting <unk> so here the computer has successfully found areas for example <unk> angles <unk> 
JeremyHoward_2014X 1 JeremyHoward_2014X 837.58 843.92 <NA> so as we go through this process we 're gradually telling the computer more and more about the kinds of structures we 're looking for 
JeremyHoward_2014X 1 JeremyHoward_2014X 843.67 849.99 <NA> you can imagine in a diagnostic test this would be a pathologist identifying areas of pathosis for example <unk> or a
JeremyHoward_2014X 1 JeremyHoward_2014X 855.02 863.29 <NA> <unk> sometimes it can be difficult for the algorithm <unk> in this case it got kind of confused the fronts and the backs of the cars are all mixed up <unk> so here we have to be a bit more careful <unk> 
JeremyHoward_2014X 1 JeremyHoward_2014X 863.29 868.36 <NA> manually selecting these fronts as opposed to the backs then telling the computer
JeremyHoward_2014X 1 JeremyHoward_2014X 868.16 871.41 <NA> that <unk> this is a type of
JeremyHoward_2014X 1 JeremyHoward_2014X 871.13 879.88 <NA> group that we 're interested in so we do that for a while we skip over a little bit <unk> and then we train the machine learning algorithm based on these couple of
JeremyHoward_2014X 1 JeremyHoward_2014X 879.81 890.05 <NA> <unk> hundred things and we hope that it 's gotten a lot better you can see it 's now started to fade some of these pictures out showing us that it already is recognizing how to understand some of these itself 
JeremyHoward_2014X 1 JeremyHoward_2014X 889.77 893.32 <NA> we can then use this concept of similar images <unk> 
JeremyHoward_2014X 1 JeremyHoward_2014X 892.83 899.99 <NA> and using similar images you can now see the computer at this point is able to entirely find just the fronts of cars so at this
JeremyHoward_2014X 1 JeremyHoward_2014X 902.83 911.27 <NA> <unk> you 've done a good job of that sometimes of course <unk> even at this point it 's still difficult to separate out groups
JeremyHoward_2014X 1 JeremyHoward_2014X 911.11 913.38 <NA> in this case even after we
JeremyHoward_2014X 1 JeremyHoward_2014X 913.14 920.18 <NA> let the computer try to rotate this for a while <unk> we still find that the left sides and the right sides pictures are all mixed up together <unk> 
JeremyHoward_2014X 1 JeremyHoward_2014X 919.81 929.4 <NA> so we can again give the computer some hints and we say okay try and find a projection that separates out the left sides and the right sides as much as possible using this deep learning algorithm
JeremyHoward_2014X 1 JeremyHoward_2014X 930.05 938.88 <NA> <unk> and giving it that hint ah <unk> okay it 's been successful it 's managed to find a way of thinking about these objects that 's separated out these together <unk> so
JeremyHoward_2014X 1 JeremyHoward_2014X 938.42 943.35 <NA> you get the idea here <unk> this is a case not where <unk>
JeremyHoward_2014X 1 JeremyHoward_2014X 946.69 954.97 <NA> the human is being replaced by a computer but where they 're working together what we 're doing here is we 're replacing something that used to take a team of five
JeremyHoward_2014X 1 JeremyHoward_2014X 954.96 963.4 <NA> <unk> or six people about seven years and replacing it with something that takes fifteen minutes for one person acting alone <unk> so
JeremyHoward_2014X 1 JeremyHoward_2014X 963 973.27 <NA> this process takes about four or five iterations you can see we now have sixty two percent of our one point five million images classified correctly <unk> and at this point we can start to quite quickly <unk>
JeremyHoward_2014X 1 JeremyHoward_2014X 973.22 979.82 <NA> grab whole big sections <unk> check through them to make sure that there 's no mistakes where there are mistakes we can let the computer know about them
JeremyHoward_2014X 1 JeremyHoward_2014X 980.83 989.69 <NA> <unk> and using this kind of process for each of the different groups we are now up to an eighty percent success rate in classifying the one point five million images
JeremyHoward_2014X 1 JeremyHoward_2014X 989.56 996.98 <NA> <unk> and at this point it 's just a case of finding the small number that aren 't classified correctly <unk> and trying to understand
JeremyHoward_2014X 1 JeremyHoward_2014X 996.64 1004.43 <NA> why <unk> and using that approach by fifteen minutes we get to ninety seven percent classification rates so
JeremyHoward_2014X 1 JeremyHoward_2014X 1004.87 1014.66 <NA> <unk> this kind of technique could allow us to fix a major problem which is that there 's a lack of medical expertise in the world the world economic forum says that there 's between a 10x and <unk>
JeremyHoward_2014X 1 JeremyHoward_2014X 1015.7 1021.35 <NA> <unk> of physicians in the developing world and it would take about three hundred years to train enough people
JeremyHoward_2014X 1 JeremyHoward_2014X 1021.35 1029.82 <NA> to fix that problem <unk> so imagine if we can help enhance their efficiency using these deep learning approaches so i 'm very excited about the
JeremyHoward_2014X 1 JeremyHoward_2014X 1029.78 1033.01 <NA> <unk> opportunities i 'm also concerned about the problems <unk> 
JeremyHoward_2014X 1 JeremyHoward_2014X 1033.01 1041.73 <NA> the problem here is that every area in blue on this map is somewhere where services are over eighty percent of employment what are services 
JeremyHoward_2014X 1 JeremyHoward_2014X 1041.54 1043.56 <NA> these are services 
JeremyHoward_2014X 1 JeremyHoward_2014X 1043.43 1047.34 <NA> these are also the exact things that computers have just learned how to do <unk> 
JeremyHoward_2014X 1 JeremyHoward_2014X 1047.36 1048.76 <NA> so
JeremyHoward_2014X 1 JeremyHoward_2014X 1048.6 1054.97 <NA> eighty percent of the world 's employment in the developed world is stuff that computers have just learned how to do what does that mean
JeremyHoward_2014X 1 JeremyHoward_2014X 1055.07 1060.29 <NA> well it 'll be fine they 'll be replaced by other jobs for example there will be more jobs for data scientists 
JeremyHoward_2014X 1 JeremyHoward_2014X 1060.29 1067.3 <NA> well not really it doesn 't take data scientists very long to build these things for example these four algorithms were all built by the same guy <unk> 
JeremyHoward_2014X 1 JeremyHoward_2014X 1067.35 1070.6 <NA> so if you think oh it 's all happened before
JeremyHoward_2014X 1 JeremyHoward_2014X 1070.6 1078.25 <NA> <unk> we 've seen the results in the past of when new things come along and they get replaced by new jobs what are these new jobs going to be <unk> 
JeremyHoward_2014X 1 JeremyHoward_2014X 1077.91 1080 <NA> it 's very hard for us to estimate
JeremyHoward_2014X 1 JeremyHoward_2014X 1080.02 1088.82 <NA> <unk> this because human performance grows at this gradual rate but we now have a system deep learning that we know actually grows in capability exponentially <unk> 
JeremyHoward_2014X 1 JeremyHoward_2014X 1088.6 1094.79 <NA> and we 're here <unk> so currently we see the things around us and we say oh computers are still pretty dumb 
JeremyHoward_2014X 1 JeremyHoward_2014X 1094.36 1102.29 <NA> right but in five years ' time computers will be off this chart <unk> so we need to be starting to think about this capability right now 
JeremyHoward_2014X 1 JeremyHoward_2014X 1101.95 1104.91 <NA> we have seen this once before of course in the industrial
JeremyHoward_2014X 1 JeremyHoward_2014X 1104.84 1109.23 <NA> <unk> revolution <unk> we saw a step change in capability thanks to engines <unk> 
JeremyHoward_2014X 1 JeremyHoward_2014X 1109.46 1119.73 <NA> the thing is though that after a while things flattened out there was social disruption but once engines were used to generate power in all the situations things really settled down 
JeremyHoward_2014X 1 JeremyHoward_2014X 1120.08 1124.84 <NA> the machine learning revolution is going to be very different from the industrial revolution <unk> because the machine
JeremyHoward_2014X 1 JeremyHoward_2014X 1126.24 1134.98 <NA> <unk> it never settles down <unk> the better computers get at intellectual activities the more they can build better computers to be better at intellectual capabilities <unk> 
JeremyHoward_2014X 1 JeremyHoward_2014X 1134.98 1142.72 <NA> so this is going to be a kind of change that the world has actually never experienced before <unk> so your previous understanding of what 's possible is different <unk> 
JeremyHoward_2014X 1 JeremyHoward_2014X 1142.56 1149.78 <NA> this is already impacting us in the last twenty five years as capital productivity has increased <unk> labor productivity has been
JeremyHoward_2014X 1 JeremyHoward_2014X 1150.34 1152.25 <NA> <unk> in fact even a little bit down
