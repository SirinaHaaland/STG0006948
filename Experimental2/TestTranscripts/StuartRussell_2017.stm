 one of the world 's greatest go players and he 's having what my friends in silicon valley call a holy cow moment 
a moment where we realize that ai is actually progressing a lot faster than we expected
 so humans have lost on the go board what about the real world well  the real world is much bigger much more complicated than the go board it
's a lot less visible  but it 's still a decision problem  
and if we think about some of the technologies that are coming down the pike
noriko [ arai ] mentioned that reading is not yet happening in machines at least with understanding  but that will happen
 and when that happens very soon afterwards machines will have read
everything that the human race has ever written  
and that will enable machines along with the ability to look further ahead than humans can as we 've already seen in go 
if they also have access to more information they 'll be able to make better decisions in the real world than we can 
so is that a good thing
 well  i hope so 
our entire civilization everything that we value is based on our intelligence  
and if we had access to a lot more intelligence  
then there 's really no limit to what the human race can do  
 and i think this could be as some people have described it the biggest event in human history  
so why are people saying things like this that ai might spell the end of the human race 
is this a new thing  is it just elon musk and bill gates and stephen hawking 
actually no this idea has been around for a while
 even if we could keep the machines in a subservient position 
for instance by turning off the power at strategic moments and i 'll come back to that turning off the power idea later on we should as a species feel greatly humbled 
so who said this this is alan turing in 
alan turing as you know is the father of computer science and in many ways the father of ai as well  so if we think about this problem
 problem of creating something more intelligent than your own species we might call this the gorilla problem 
because gorillas ' ancestors did this a few million years ago  and now we can ask the gorillas 
was this a good idea  so here they are having a meeting to discuss whether it was a good idea and
after a little while they conclude no this was a terrible idea our species is in dire straits
in fact you can see the existential sadness in their eyes so this queasy feeling that
making something smarter than your own species is is maybe not a good idea
what can we do about that  well really nothing  except stop doing ai
and because of all the benefits that i mentioned and because i 'm an ai researcher i 'm not
 having that 
i actually want to be able to keep doing ai 
so we actually need to nail down the problem a bit more what exactly is the problem why is better ai possibly a catastrophe  
so here 's another quotation 
we had better be quite sure that the purpose put into the machine is the purpose which we really desire this was said by norbert wiener
in one thousand nine hundred and sixty shortly after he watched one of the very early learning systems learn to play checkers better than its creator  
but this could equally have been said by king midas
king midas said i want everything i touch to turn to gold and he got exactly what he asked for that was the purpose that he put into the machine  so to speak
 then his food and his drink and his relatives turned to gold and he died in misery and starvation 
so we 'll call this the king midas problem 
of stating an objective which is not in fact  truly aligned with what we want in modern terms we call this the value alignment problem
 putting in the wrong objective is not the only part of the problem there 's another part if you put an objective into a machine even something as simple as fetch the coffee 
the machine says to itself 
well how might i fail to fetch the coffee  someone might switch me off  
ok i have to take steps to prevent that i will disable my ' off ' switch  i will do anything to defend myself against interference with this objective that i have been given
so this  single minded pursuit
in a very defensive mode of an objective that is in fact  not aligned with the true objectives of the human race
that 's the problem that we face  and in fact that 's the 
 from this talk  if you want to remember one thing  it 's that you can 't fetch the coffee if you 're dead
 simple just remember that repeat it to yourself three times a day 
and in fact this is exactly the plot
of 
two thousand and one [ a space odyssey ] hal has an objective a mission which is not aligned with the objectives of the humans and that leads to this conflict 
now fortunately
 he 's pretty smart  but eventually dave outwits him and manages to switch him off  
but we might not be so lucky  
so what are we going to do 
i 'm trying to redefine ai to
 get away from this classical notion of machines that
intelligently pursue objectives there are three principles involved the first one
is a principle of altruism if you like that the robot 's only objective
is to maximize the realization of human objectives of human values and by values here i don 't mean 
 values i just mean whatever it is that the human would prefer their life to be like
and so this actually violates asimov 's law that the robot has to protect its own existence it has no interest in preserving its existence whatsoever  
the second law is a law of humility if you like  and this turns out to be really important to make robots safe  
 it says that the robot does not know what those human values are so it has to maximize them  but it doesn 't know what they are 
and that avoids this problem of single minded pursuit of an objective  this uncertainty turns out to be crucial  
now in order to be useful to us it has to have some idea of what we want 
it obtains that information primarily by observation of human choices so our own choices reveal information
about what it is that we prefer our lives to be like  
so those are the three principles let 's see how that applies to this question of can you switch the machine off as turing suggested  
so here 's a pr2 robot this is one that we have in our lab and it has a big red off switch
 is it going to let you switch it off  if we do it the classical way we give it the objective of fetch the coffee i must fetch the coffee  i can 't fetch the coffee
if i 'm dead so obviously the pr2 has been listening to my talk  and so it says therefore i must disable my ' off ' switch 
and probably taser all the other people in starbucks who might interfere with me 
 this seems to be inevitable right this kind of of failure mode seems to be inevitable and it follows from having a concrete definite objective  
so what happens if the machine is uncertain about the objective well it reasons in a different way it says ok the human might switch me off but
only if i 'm doing something wrong well i don 't really know what wrong is but i know that i don 't want to do it so that 's the first and second principles right there
i should let the human switch me off 
and in fact you can calculate the incentive that the robot has to allow the human to switch it off  
and it 's directly tied to the degree of uncertainty about the underlying objective  
and then when the machine
is switched off  that third principle comes
 it learns something about the objectives it should be pursuing because it learns that what it did wasn 't right
in fact we can with suitable use of greek symbols as mathematicians usually do  we can actually prove a theorem
that says that such a robot is provably beneficial to the human you are provably better off with a machine that 's designed in this way than without it 
so this is a very simple example but this is the first step in in what we 're trying to do with
now 
this third principle i think is the one that you 're probably scratching your head over you 're probably thinking well you know  
i behave badly  i don 't want my
my robot to behave like me i sneak down in the middle of the night and take stuff from the fridge i do this and that there 's all kinds of things you don 't want the robot doing
 but in fact it doesn 't quite work that way just because you
 behave badly
doesn 't mean the robot is going to copy your behavior it 's going to understand your motivations
and maybe help you resist them  if appropriate  
but it 's still difficult what we 're trying to do in fact  is to allow machines to predict for any person
and for any possible life that they could live  and the lives of everybody else
 which would they prefer 
and there are many many difficulties involved in doing this i don 't expect that this is going to get solved very quickly 
the real difficulties in fact  are us  
as i have already mentioned  we behave badly in fact some of us are downright nasty
 now the robot as i said doesn 't have to copy the behavior  the robot does not have any objective of its own it 's purely altruistic  
and it 's not designed just to satisfy the desires of one person the user  but in fact it has to respect the preferences of everybody  
so it can deal with a certain amount of nastiness and it can even understand that your nastiness for
 you may take bribes as a passport official
because you need to feed your family and send your kids to school 
it can understand that it doesn 't mean it 's going to steal  in fact it 'll just help you send your kids to school 
we are also computationally limited
 a brilliant go player but he still lost so if we look at his actions he took an action that lost the game  that doesn 't mean he wanted to lose 
so to understand his behavior
 we actually have to invert through a model of human cognition that includes our computational limitations a very complicated model  
but it 's still something that we can work on understanding  
probably the most difficult part from my point of view as an ai researcher is the fact that there are lots of us 
 and so the machine has to somehow trade off weigh up the preferences of many different people 
and there are different ways to do that  economists sociologists  moral philosophers have understood that and we are actively looking for collaboration let 's have a look and see what happens when you get that wrong 
so you can have a conversation for example with your intelligent personal assistant that might be available
in a few years ' time think
 your wife called to remind you about dinner tonight and of course you 've forgotten what what dinner  what are you talking about
 well what am i going to do i can 't just tell him i 'm too busy don 't worry  i arranged for his plane to be delayed 
some kind of computer malfunction really you can do that 
he sends his profound apologies and looks forward to meeting you for lunch tomorrow
 's a slight mistake going on  this is clearly following my wife 's values which is happy wife happy life 
it could go the other way  you could come home after a hard day 's work
 there are humans in south sudan who are in more urgent need than you so i 'm leaving make your own dinner 
so we have to solve these problems  and i 'm looking forward to working on them  
there are reasons for optimism one reason is there is a massive amount of data because remember i said they 're going to read everything the human race has ever written
 most of what we write about is human beings doing things
and other people getting upset about it so there 's a massive amount of data to learn from there 's also a very strong economic incentive
to get this right  so imagine your domestic robot 's at home you 're late from work again
and the robot has to feed the kids and the kids are hungry and there 's nothing in the fridge  and the robot
sees the cat
 and the robot hasn 't quite learned the human value function properly so it doesn 't understand the sentimental value of the cat outweighs the nutritional value of the cat  
so then what happens well   
it happens like this deranged robot cooks kitty for family dinner
 that one incident would be the end of the domestic robot industry  so there 's a huge incentive to get this right
long before we reach superintelligent machines  
so to summarize 
i 'm actually trying to change the definition of ai so that we have provably beneficial machines  and the principles are machines that are altruistic  that want to achieve only our objectives  
but that are uncertain about what those objectives are and will watch all of us
to learn more about what it is that we really want 
and hopefully in the process we will learn to be better people  thank you very much chris anderson so interesting 
stuart we 're going to stand here a bit because i think they 're setting up for our next speaker a couple of questions
 the idea of programming in ignorance seems intuitively really powerful  as you get to 
what 's going to stop a robot reading literature and discovering this idea that knowledge is actually better than ignorance and still just shifting its own goals and rewriting that
 as it becomes more
correct so the the evidence is there and  and it 's going to be designed to interpret it correctly it will understand for example that books
are very biased in the in the evidence they contain they only talk about kings and princes and elite white male people doing stuff  so it 's a complicated problem
 it learns more about our objectives it will become more and more useful to us ca and you couldn 't just boil it down to one law you know  
hardwired in if any human ever tries to switch me off  i comply i comply sr 
absolutely not  that would be a terrible idea so imagine that you have a self driving car
and you want to send your five year old off to preschool
 to be able to switch off the car while it 's driving along probably not so it needs to understand
how rational and sensible the person is the more rational the person  the more willing you are to be switched off if the person is completely random or even malicious then you 're less willing to be switched off ca all
right stuart can i just say i really really hope you figure this out for us thank you so much for that
